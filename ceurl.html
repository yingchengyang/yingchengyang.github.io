<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Fourier Controller Networks for Real-Time Decision-Making in Embodied Learning">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Fourier Controller Networks for Real-Time Decision-Making in Embodied Learning</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://github.com/thu-ml/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Fourier Controller Networks for Real-Time Decision-Making in Embodied Learning</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://github.com/thkkk">Hengkai Tan</a><sup>1</sup>,</span>
            <span class="author-block">
              <a>Songming Liu</a><sup>1</sup>,</span>
            <span class="author-block">
              <a>Kai Ma</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a>Chengyang Ying</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a>Xingxing Zhang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a>Hang Su</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a>Jun Zhu</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Tsinghua University</span>
            <!-- <span class="author-block"><sup>2</sup>?</span> -->
          </div>

          <div class="is-size-5 publication-authors">
            <span style="font-weight: bold;">ICML 2024</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2405.19885"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2405.19885"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/thkkk/FCNet"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code & Data</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
              </span> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- 
<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Transformer has shown promise in reinforcement learning to model time-varying 
            features for obtaining generalized low-level robot policies on diverse robotics 
            datasets in embodied learning. However, it still suffers from the issues of low data efficiency and high inference latency.
          </p>
          <p>
            In this paper, we propose to investigate the task from a new perspective of the frequency domain.
             We first observe that the energy density in the frequency domain of a robot's trajectory is 
             mainly concentrated in the low-frequency part. Then, we present the Fourier Controller Network
              (FCNet), a new network that uses Short-Time Fourier Transform (STFT) to extract and encode
               time-varying features through frequency domain interpolation.
            In order to do real-time decision-making, we further adopt FFT and Sliding DFT methods 
            in the model architecture to achieve parallel training and efficient recurrent inference.
          </p>
          <p>
            Extensive results in both simulated (e.g., D4RL) and real-world environments (e.g., 
            robot locomotion) demonstrate FCNet's substantial efficiency and effectiveness over existing
             methods such as Transformer, e.g., FCNet outperforms Transformer on multi-environmental 
             robotics datasets of all types of sizes (from 1.9M to 120M).
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <!-- <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div> -->
        <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/full_video.mp4"
                    type="video/mp4">
          </video>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>

</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Crossing knee-deep snow</h2>
          <p>
            For scenes that have <b>never been seen in the dataset</b> (which is is collected in simulator), 
            such as knee-deep snow, 
            FCNet policy can show good locomotion performance, robustness and generalization.
          </p>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/deep_snow.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <div class="column">
        <h2 class="title is-3">Dashing in the snow</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
                The FCNet policy slammed through the snow with ease. 
                We find that the FCNet policy <b>lasts longer and has less joint 
                heating</b> than the traditional RL policy, which may be due to the fact 
                that FCNet filters out the high-frequency noise, making the output action softer.
            </p>
            <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/dash.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div>
    <p>
      We find that a limited amount of simulator data (e.g. 60M steps or less) can cover a very large 
      amount of real-world terrain, so we expect to utilize a large amount of cheap simulator 
      data in the future to lead to scaling laws for embodied AI, including robotic manipulation.
    </p>
    <!--/ Matting. -->

    <!-- methods images -->
    <div class="row">
      <!-- sequence modeling. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Methods</h2>
          <p>
            One common feature of the existing works on Transformer for decision making is that they 
            mainly focus on modeling 
            the time-varying features of robotic trajectories in <b>time domain</b> by drawing a direct 
            analogy with that in modeling natural language sentences. We argue that this is insufficient in 
            embodied learning, which has its unique features.

            Specifically, we take a close examination on embodied learning from the <b>frequency domain</b>
            and observe that the energy density distribution of a robot's state sequence is mainly concentrated
             in the low-frequency part, as shown in figure below. This is due to the 
             inherent continuity and smoothness in natural physical phenomena and robot motors.
          </p>
          <p style="text-align:center;">
            <!-- small image -->
            <image src="./static/images/plot_energy_merged.png" class="img-responsive" style="width: 50%;">
          </p>

          <p>
            However, by directly modeling trajectories in the time domain, existing works on Transformer and 
            its variants do not take into account this inductive bias for robotic control, resulting in 
            low data efficiency and high inference latency.
            To address these issues, we propose a new architecture of Fourier Controller Network (FCNet) 
            based on the key observation in the frequency domain. 
            FCNet grounds the inductive bias in robotic control inspired by the Fourier transform.

            We conceptualize low-level continuous control as a sequential decision-making problem. 
            Our neural model is adept at predicting subsequent actions by analyzing a historical window 
            of state data, as depicted in the figure below.
            We concatenate action, reward, and state tokens together and then perform parallel training and 
            real-time decision inference on this sequence.
            <!-- Especially in a continuous state 
            space like embodied learning, it makes sense to perform Fourier-based modeling and
             training in the frequency domain of time-domain sequences within a window [t, t+n-1].
              In the inference phase T=t+n-1 to T=t+n, the sliding window in 
              Sec. <i>Reprensentation of Recurrent Inference</i> is utilized for efficient inference.  -->
          </p>
          <p style="text-align:center;">
            <image src="./static/images/sequence_model.png" class="img-responsive" style="width: 70%;">
          </p>

          
          <p>
            Guided by the observation in the frequency domain and the inductive reasoning that 
            differential dynamics are simplified in the frequency domain, FCNet introduces a causal
            spectral convolution (CSC) block.  It employs the Short-Time Fourier Transform (STFT) and 
            linear transform 
            for efficient feature extraction in the <b>frequency domain</b>, distinct from Transformer 
            and other prevalent architectures.

            As shown in figure below, we focus on the $m$ lowest modes, with $m$ strategically selected to 
            be $\ll n$, where $n$ is the length of the state window. 
            Consequently, the high-frequency part in the frequency domain is filtered, allowing us to 
            focus solely on these $m$ lowest modes. 
            The CSC makes efficient training and real-time inference possible, and has also been 
            shown to have good  performance in experiments. 
          </p>
          <p style="text-align:center;">
            <image src="./static/images/model_arch.png" class="img-responsive">
          </p>
          <p>
            Furthermore, to achieve efficient parallel training and inference, 
            which necessitates causality in the model's sequential outputs (dependent only on previous inputs)
            and the rapid generation of each output token for real-time response,
            we introduce parallel training based on Fast Fourier transform (FFT), 
            and recurrent inference based on sliding discrete Fourier transform (Sliding DFT) in FCNet.
          </p>
          <p style="text-align:center;">
            <image src="./static/images/comparison.png" class="img-responsive">
          </p>
          <p>
            This efficiency marks a significant speed advantage over traditional Transformer models,
             enabling handling the complexities of real-time continuous control in dynamic environments.
          </p>
        </div>
      </div>
      <!--/ sequence modeling. -->
    </div>
  
    <!-- methods images -->
    <div class="row">
      <!-- sequence modeling. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Some experimental results</h2>
          <p>
            Comprehensive analyses in both simulated (e.g., D4RL) and real-world environments 
            (e.g., robot locomotion) demonstrate FCNet's substantial efficiency and effectiveness 
            over existing methods such as Transformer, RetNet.
            FCNet outperforms Transformer on multi-environmental robotics datasets of 
            all types of sizes (from 1.9M to 120M).
            The results show that FCNet significantly outperforms Transformer with limited data.
          </p>
          <p style="text-align:center;">
            <image src="./static/images/plot_data_size.png" class="img-responsive" style="width: 70%;">
          </p>

          
          <p>
            We test the inference latency of the Transformer (with KV cache) and FCNet under different
             hyperparameter settings related to model structure. The results show that the upward curve 
             of the inference latency of FCNet is significantly slower than that of Transformer as the 
             context length, number of layers, and hidden size are improved. This demonstrates the efficiency
             of the inference of FCNet.
          </p>
          <p style="text-align:center;">
            <image src="./static/images/plot_only_arch_speed.png" class="img-responsive">
          </p>
        </div>
      </div>
      <!--/ sequence modeling. -->
    </div>

    <!-- Concurrent Work. -->
    <!-- <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div> -->
    <!--/ Concurrent Work. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{tanfourier,
      title={Fourier Controller Networks for Real-Time Decision-Making in Embodied Learning},
      author={Tan, Hengkai and Liu, Songming and Ma, Kai and Ying, Chengyang and Zhang, Xingxing and Su, Hang and Zhu, Jun},
      booktitle={Forty-first International Conference on Machine Learning}
    }</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/pdf/2405.19885">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/thkkk" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
