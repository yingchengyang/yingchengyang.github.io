<!DOCTYPE html>
<html>

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<head>
  <meta charset="utf-8">
  <meta name="description"
        content="An Useful Lemma for Several RL Results.">
  <meta name="keywords" content="Reinforcement Learning, Policy Gradient, Max-Entropy RL">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>An Useful Lemma for Several RL Results</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="../../static/css/bulma.min.css">
  <link rel="stylesheet" href="../../static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="../../static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="../../static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="../../static/css/index.css">
  <link rel="icon" href="../../static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="../../static/js/fontawesome.all.min.js"></script>
  <script src="../../static/js/bulma-carousel.min.js"></script>
  <script src="../../static/js/bulma-slider.min.js"></script>
  <script src="../../static/js/index.js"></script>
</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">A Useful Lemma for Several RL Results</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block"> 
              <!-- <sup>1</sup> -->
              <a href="https://yingchengyang.github.io/">Chengyang Ying</a></span>
          </div>

          <!-- <div class="is-size-5 publication-authors">
            <span class="author-block">Tsinghua University</span>
            <span class="author-block"><sup>2</sup>Google Research</span>
          </div> -->

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://www.zhihu.com/people/ying-cheng-yang"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>知乎</span>
                </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits.
      </h2>
    </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
    <h2 class="title is-3">1. Overview</h2>
    <div class="content has-text-justified">
    <p>强化学习里面有很多经典的理论结果，比如策略梯度定理，两个策略的累计期望奖励的差等等，这个blog介绍一个统一的工具来分析这些问题。

    考虑一个MDP $\mathcal{M}=(\mathcal{S},\mathcal{A},\mathcal{P}, \mathcal{R},\gamma,\rho_0)$和策略$\pi$，我们可以定义
    $$
    \begin{equation}
        d_{\mathcal{M}}^{\pi}(s) = (1-\gamma)\sum_{t=0}^{\infty} \gamma^t \mathcal{P}(s_t=s|\pi,\mathcal{M}).
    \end{equation}
    $$
    我们有如下引理<br>

    <b>引理：</b>对于任意$\mathcal{S}$上的函数$f,g$，如果他们满足
    $$
    \begin{equation}
    \label{eq_bell}
        f(s) = g(s) + \gamma\int \pi(a|s)\mathcal{P}(s'|s,a) f(s') dads',
    \end{equation}
    $$
    那么我们有
    $$
    \begin{equation}
    \label{eq_result}
        \mathbb{E}_{s\sim\rho_0} \left[f(s)\right] = \frac{1}{1-\gamma}\mathbb{E}_{s\sim d_{\mathcal{M}}^{\pi}} \left[g(s)\right].
    \end{equation}
    $$
    为了便于阅读，我们先看看这个引理的具体应用，然后再给出这个引理的证明 (第5小节)。
   </p>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
    <h2 class="title is-3">2. Policy Gradient [1]</h2>
    <div class="content has-text-justified">
    <p>我们取$f(s) = \nabla_{\pi} [V_{\pi,\mathcal{M}}(s)]$, 自然地，我们有
      $$
      \begin{equation}
      \begin{split}
          f(s) =& \nabla_{\pi} [V_{\pi,\mathcal{M}}(s)] = \nabla_{\pi} \left[\int \pi(a|s) Q_{\pi,\mathcal{M}}(s) ds\right] \\
          =& \underbrace{\int Q_{\pi,\mathcal{M}}(s) \nabla_{\pi} \pi(a|s) ds}_{\mathrm{defined\ as\ } g(s)}  + \int \pi(a|s) [\nabla_{\pi}  Q_{\pi,\mathcal{M}}(s)] ds \\
          =& g(s) + \int \pi(a|s) \nabla_{\pi} \left[\int \mathcal{R}(s,a) + \gamma \int \mathcal{P}(s'|s,a) V_{\pi,\mathcal{M}}(s') ds' da\right] ds \\
          =& g(s) + \int \pi(a|s)  \left[ \gamma \int \mathcal{P}(s'|s,a) \nabla_{\pi} V_{\pi,\mathcal{M}}(s') ds' da\right] ds \\
          =& g(s) + \gamma \int \pi(a|s)  \mathcal{P}(s'|s,a) \underbrace{\nabla_{\pi} V_{\pi,\mathcal{M}}(s')}_{f(s')} ds' da ds.
      \end{split}
      \end{equation}
      $$
      因此我们就可以得到
      $$
      \begin{equation}
      \begin{split}
          \nabla_{\pi} J_{\mathcal{M}}(\pi) = & \nabla_{\pi} \mathbb{E}_{s\sim\rho_0} [V_{\mathcal{M}, \pi}(s)] =  \mathbb{E}_{s\sim\rho_0} [\nabla_{\pi} V_{\mathcal{M}, \pi}(s)] =  \mathbb{E}_{s\sim\rho_0} [f(s)] \\
          = & \frac{1}{1-\gamma}\mathbb{E}_{s\sim d_{\mathcal{M}}^{\pi}} [g(s)] =\frac{1}{1-\gamma}\mathbb{E}_{s\sim d_{\mathcal{M}}^{\pi}} \int [\nabla_{\pi} \pi(a|s)] Q_{\pi,\mathcal{M}}(s) ds \\
          = & \frac{1}{1-\gamma}\mathbb{E}_{s\sim d_{\mathcal{M}}^{\pi}, a\sim\pi(\cdot|s)} [Q_{\pi,\mathcal{M}}(s) \nabla_{\pi} \log\pi(a|s)],
      \end{split}
      \end{equation}
      $$
      这样我们就证明了策略梯度定理（Policy Gradient）。
   </p>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
    <h2 class="title is-3">3. 不同策略/MDP上累计期望奖励之差</h2>
    <div class="content has-text-justified">
    <p>一个经常被考虑的问题是不同策略在不同MDP下的累计期望奖励的差，即给定另一个MDP $\mathcal{M}'$和策略$\pi'$，我们考虑$J_{\mathcal{M'}}(\pi') - J_{\mathcal{M}}(\pi)$。

      我们取$f(s) = V_{\pi',\mathcal{M}'}(s) - V_{\pi,\mathcal{M}}(s)$,自然地，我们有
      $$
      \begin{equation}
      \begin{split}
          & f(s) = V_{\pi',\mathcal{M}'}(s) - V_{\pi,\mathcal{M}}(s)\\
          = & V_{\pi',\mathcal{M}'}(s)
          - \int \pi(a|s) \left[\mathcal{R}(s,a) +  \gamma\mathcal{P}(s'|s,a)V_{\pi, \mathcal{M}}(s')\right]dads' \\
          = & \underbrace{V_{\pi',\mathcal{M}'}(s) - \int \pi(a|s) \left[\mathcal{R}(s,a) +  \gamma\mathcal{P}(s'|s,a)V_{\pi', \mathcal{M}'}(s')\right] dads'}_{\mathrm{defined\ as\ } g(s)} \\
          + & \gamma \int \pi(a|s)    \mathcal{P}(s'|s,a)\underbrace{\left[V_{\pi', \mathcal{M}'}(s') - V_{\pi, \mathcal{M}}(s')\right]}_{f(s')} dads'.
      \end{split}
      \end{equation}
      $$
      因此我们可以得到
      $$
      \begin{equation}
      \begin{split}
          & J_{\mathcal{M'}}(\pi') - J_{\mathcal{M}}(\pi) = \mathbb{E}_{s\sim \rho_0} [V_{\pi',\mathcal{M}'}(s) - V_{\pi,\mathcal{M}}(s)] =  \mathbb{E}_{s\sim\rho_0} [f(s)] 
          = \frac{1}{1-\gamma}\mathbb{E}_{s\sim d_{\mathcal{M}}^{\pi}} [g(s)] \\
          % = &\frac{1}{1-\gamma} \int d_{\mathcal{M}}^{\pi}(s) \left[V_{\pi',\mathcal{M}'}(s) - \pi(a|s) \left[\mathcal{R}(s,a) +  \gamma\mathcal{P}(s'|s,a)V_{\pi', \mathcal{M}'}(s')\right]\right]  dsdads'.
      \end{split}
      \end{equation}
      $$
      
      特别的，当$\mathcal{M}'=\mathcal{M}$时，$g(s)$可以简化为：
      $$
      \begin{equation}
      \begin{split}
      \label{eq_same_mdp}
          g(s) = & V_{\pi',\mathcal{M}}(s) - \int \pi(a|s) \left[\mathcal{R}(s,a) +  \gamma\mathcal{P}(s'|s,a)V_{\pi', \mathcal{M}}(s')\right] dads' \\
          = & V_{\pi',\mathcal{M}}(s) - \int \pi(a|s) Q_{\pi', \mathcal{M}}(s,a) da \\
          \overset{(*)}{=} & \int (\pi'(a|s) - \pi(a|s)) Q_{\pi', \mathcal{M}}(s,a) da \\
          = & \int (\pi'(a|s) - \pi(a|s)) Q_{\pi', \mathcal{M}}(s,a) da - \underbrace{\int (\pi'(a|s) - \pi(a|s)) V_{\pi', \mathcal{M}}(s) da}_{=0}  \\
          = & \int (\pi'(a|s) - \pi(a|s)) A_{\pi', \mathcal{M}}(s,a) da   \\
          = & \int (\pi'(a|s) - \pi(a|s)) A_{\pi', \mathcal{M}}(s,a) da - \underbrace{\int \pi'(a|s) A_{\pi', \mathcal{M}}(s,a) da}_{=0} \\
          = & - \int  \pi(a|s) A_{\pi', \mathcal{M}}(s,a) da .
      \end{split}
      \end{equation}
      $$
      因此我们得到
      $$
      \begin{equation}
      \begin{split}
          J_{\mathcal{M}}(\pi') - J_{\mathcal{M}}(\pi) = \frac{-1}{1-\gamma} \int d_{\mathcal{M}}^{\pi}(s) \pi(a|s) A_{\pi', \mathcal{M}}(s,a) dsda.
      \end{split}
      \end{equation}
      $$
      这就是文章[2]中的引理6.1以及TRPO [3]中的式子(2)。<br>
      
      同时，我们也可以基于式子$(*)$证明贪心策略的策略提升（Policy Improvement），我们有
      $$
      \begin{equation}
      \begin{split}
          J_{\mathcal{M}}(\pi') - J_{\mathcal{M}}(\pi) = \frac{1}{1-\gamma} \int d_{\mathcal{M}}^{\pi}(s) (\pi'(a|s) - \pi(a|s)) Q_{\pi', \mathcal{M}}(s,a) ds da,
      \end{split}
      \end{equation}
      $$
      交换一下$\pi$和$\pi'$可以得到
      $$
      \begin{equation}
      \begin{split}
          J_{\mathcal{M}}(\pi) - J_{\mathcal{M}}(\pi') = \frac{1}{1-\gamma} \int d_{\mathcal{M}}^{\pi'}(s) (\pi(a|s) - \pi'(a|s)) Q_{\pi, \mathcal{M}}(s,a) ds da \leq 0,
      \end{split}
      \end{equation}
      $$
      这里最后一个小于等于号是因为$\pi'(s) = \arg\max_a Q_{\pi,\mathcal{M}}(s,a)$。
      
      除此之外，我们也可以得到$J_{\mathcal{M}}(\pi') - J_{\mathcal{M}}(\pi)$的一些上界，为了不影响阅读的连贯性，我们把这些内容放到第6小节。
   </p>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
    <h2 class="title is-3">4. Max-Entropy RL上的策略提升</h2>
    <div class="content has-text-justified">
    <p>
      Max-Entropy RL是强化学习中一类重要的问题，包括Soft Q Learning [4]，SAC等算法。这些方法主要考虑在最大化奖励的同时鼓励一定的探索，即优化目标为
      $$
      \begin{equation}
      \begin{split}
          J_{\mathrm{ME}}(\pi) \triangleq & \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t \left(\mathcal{R}(s_t, a_t) + \alpha \mathcal{H}(\pi(\cdot|s_t)) \right)\right]
          = J(\pi) + \frac{\alpha}{1-\gamma} \mathbb{E}_{s\sim d_{\pi}}  \left[\mathcal{H}(\pi(s)) \right]. 
      \end{split}
      \end{equation}
      $$
      我们定义Soft Q函数为
      $$
      \begin{equation}
      \begin{split}
          Q'_{\pi,\mathcal{M}}(s, a) & = \mathbb{E}\left[ \mathcal{R}(s, a) + \sum_{i=1}^{\infty} \gamma^i \left(\mathcal{R}(s_i, a_i) +  \alpha \mathcal{H}(\pi(\cdot|s_i)) \right)  \right].
      \end{split}
      \end{equation}
      $$
      每次策略更新为
      $$
      \begin{equation}
          \pi'(\cdot|s) \propto \exp (Q'_{\pi,\mathcal{M}}(s,\cdot) / \alpha).
      \end{equation}
      $$
      我们希望证明这种策略更新可以实现策略提升，即$J_{\mathrm{ME}}(\pi') \ge J_{\mathrm{ME}}(\pi)$.
      首先，为了简化符号，我们定义
      $$
      \begin{equation}
      \begin{split}
          F(\mu, Q, s) = \mathbb{E}_{a\sim\mu(\cdot|s)}  \left[ Q(s, a) + \alpha \mathcal{H}(\mu(\cdot|s)) \right].
      \end{split}
      \end{equation}
      $$
      我们可以证明
      $$
      \begin{equation}
      \begin{split}
          Q'_{\pi,\mathcal{M}}(s, a) 
          % = &\mathcal{R}(s, a) + \mathbb{E}\left[  \sum_{i=1}^{\infty} \gamma^i \left(\mathcal{R}(s_i, a_i)  +  \alpha \mathcal{H}(\pi(\cdot|s_i))  \right) \Bigg{|} s_0=s, a_0=a \right] \\
          = &\mathcal{R}(s, a) + \alpha \gamma \mathbb{E}_{s_1\sim\mathcal{P}(\cdot|s,a)} \left(\mathcal{H}(\pi(\cdot|s_1)) \right) + \gamma \mathbb{E}_{s_1,a_1} \left[ Q'_{\pi,\mathcal{M}}(s_1, a_1)\right] \\
          = & \mathcal{R}(s, a) + \gamma \mathbb{E}_{s_1} [F(\pi, Q'_{\pi,\mathcal{M}}, s_1)].
      \end{split}
      \end{equation}
      $$
      我们定义
      $$
      \begin{equation}
      \begin{split}
          f(s) \triangleq & F(\pi', Q'_{\pi',\mathcal{M}}, s) - F(\pi, Q'_{\pi,\mathcal{M}}, s) \\
          =& \underbrace{F(\pi', Q'_{\pi,\mathcal{M}}, s) - F(\pi, Q'_{\pi,\mathcal{M}}, s)}_{\mathrm{defined\ as\ } g(s)}
          + F(\pi', Q'_{\pi',\mathcal{M}}, s) - F(\pi', Q'_{\pi,\mathcal{M}}, s) \\
          =& g(s) 
          +  \mathbb{E}_{a\sim\pi'(\cdot|s)}  [ Q'_{\pi',\mathcal{M}}(s, a) - Q'_{\pi,\mathcal{M}}(s, a) ] \\
          =& g(s)  
          + \gamma\mathbb{E}_{a\sim\pi'(\cdot|s), s_1} [\underbrace{F(\pi', Q'_{\pi',\mathcal{M}}, s_1) - F(\pi, Q'_{\pi,\mathcal{M}}, s_1)}_{f(s_1)}].
      \end{split}
      \end{equation}
      $$
      因此，我们可以证明
      $$
      \begin{equation}
      \begin{split}
          J_{\mathrm{ME}}(\pi') - J_{\mathrm{ME}}(\pi) = & \mathbb{E}_{s\sim\rho_0} \left[ F(\pi', Q'_{\pi',\mathcal{M}}(s,\cdot), s) - F(\pi, Q'_{\pi,\mathcal{M}}(s,\cdot), s)\right] 
          = \mathbb{E}_{s\sim\rho_0} [f(s)] \\
          = & \frac{1}{1-\gamma} \mathbb{E}_{s\sim d_{\mathcal{M}}^{\pi'}}[g(s)] \\
          = & \frac{1}{1-\gamma} \mathbb{E}_{s\sim d_{\mathcal{M}}^{\pi'}}[F(\pi', Q'_{\pi,\mathcal{M}}, s) - F(\pi, Q'_{\pi,\mathcal{M}}, s)]
      \end{split}
      \end{equation}
      $$
      最后我们证明$F(\pi', Q'_{\pi,\mathcal{M}}, s) \ge F(\pi, Q'_{\pi,\mathcal{M}}, s)$，将$F$看成关于$\mu$的函数，最优解$\mu^*$满足
      $$
      \begin{equation}
      \begin{split}
          Q(s, a) = \alpha  \log \mu^*(a|s) + b \alpha,
      \end{split}
      \end{equation}
      $$
      这里$b$是一个常数，并且我们有 $\mu^*(a|s) =  e^{\frac{Q(s, a)}{\alpha} - b}$。由于$\int \mu^*(a|s) da = 1$我们有
      $$
      \begin{equation}
      \begin{split}
          b =& \log\int  e^{\frac{Q(s, a)}{\alpha}} da, \quad 
          \mu^*(a|s) =  \frac{ e^{\frac{Q(s, a)}{\alpha}}}{\int  e^{\frac{Q(s, a')}{\alpha}} da'} \propto e^{\frac{Q(s, a)}{\alpha}}.
      \end{split}
      \end{equation}
      $$
      即
      $$
      \begin{equation}
          F(\pi', Q'_{\pi,\mathcal{M}}, s) \ge F(\pi, Q'_{\pi,\mathcal{M}}, s).
      \end{equation}
      $$
      这样我们就证明了Max-Entropy RL的策略提升定理（[4]中的定理4）。
   </p>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
    <h2 class="title is-3">5. 引理的证明</h2>
    <div class="content has-text-justified">
    <p>
      在这一小节，我们给出这个引理的证明（证明的核心思路基于文章[5]）<br>
      首先我们有
      $$
      \begin{equation}
      \begin{split}
          \mathbb{E}_{s\sim d_{\mathcal{M}}^{\pi}} [f(s)] =& \mathbb{E}_{s\sim d_{\mathcal{M}}^{\pi}} [g(s)] + \mathbb{E}_{s\sim d_{\mathcal{M}}^{\pi}} \left[ \gamma\int \pi(a|s)\mathcal{P}(s'|s,a) f(s') da ds' \right] \\
          =& \mathbb{E}_{s\sim d_{\mathcal{M}}^{\pi}} [g(s)] + \int f(s') \left[\int  \gamma d_{\mathcal{M}}^{\pi}(s) \pi(a|s)\mathcal{P}(s'|s,a) ds da  \right] ds' \\
          \overset{1}{=}& \mathbb{E}_{s\sim d_{\mathcal{M}}^{\pi}} [g(s)] + \int f(s') \left[d_{\mathcal{M}}^{\pi}(s') - (1-\gamma)\rho_0(s') \right] ds' \\
          =& \mathbb{E}_{s\sim d_{\mathcal{M}}^{\pi}} [g(s)] + \int f(s) \left[d_{\mathcal{M}}^{\pi}(s) - (1-\gamma)\rho_0(s) \right] ds \\
      \end{split}
      \end{equation}
      $$
      这里式子1是因为
      $$
      \begin{equation}
      \begin{split}
          d_{\mathcal{M}}^{\pi}(s) - (1-\gamma)\rho_0(s)
          = \gamma\sum_{s'}d_{\mathcal{M}}^{\pi}(s')\sum_{a}\pi(a|s')\mathcal{P}(s|s', a).
      \end{split}
      \end{equation}
      $$
      因此我们可以得到
      $$
      \begin{equation}
      \begin{split}
          \int f(s)  (1-\gamma)\rho_0(s)  ds = \mathbb{E}_{s\sim d_{\mathcal{M}}^{\pi}} [g(s)].
      \end{split}
      \end{equation}
      $$
      即$\mathbb{E}_{s\sim\rho_0} \left[f(s)\right] = \frac{1}{1-\gamma}\mathbb{E}_{s\sim d_{\mathcal{M}}^{\pi}} \left[g(s)\right]$.
   </p>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
    <h2 class="title is-3">6. 第3小节中的进一步上界</h2>
    <div class="content has-text-justified">
    <p>
      最后，我们给出$J_{\mathcal{M}'}(\pi') - J_{\mathcal{M}}(\pi)$的一些上界分析。<br>

      当$\mathcal{M}'=\mathcal{M}$时，基于第三小节中的$(*)$式，我们记$r^* = \max_{s,a} |\mathcal{R}(s,a)|$, $D_{\mathbf{TD}}(\pi,\pi')(s) = \frac{1}{2}\int |\pi'(a|s) - \pi(a|s)|da, \hat{V}_{\mathcal{M},\pi} = \max_s V_{\mathcal{M},\pi}(s) - \min_s V_{\mathcal{M},\pi}(s)$, $\bar{V}_{\mathcal{M},\pi} = \frac{1}{2}(\max_s V_{\mathcal{M},\pi}(s) + \min_s V_{\mathcal{M},\pi}(s))$,
      $$
      \begin{equation}
      \begin{split}
          g(s)
          = & \int (\pi'(a|s) - \pi(a|s)) Q_{\pi', \mathcal{M}}(s,a) da \\
          = & \int (\pi'(a|s) - \pi(a|s)) \mathcal{R}(s,a) da + \gamma \int (\pi'(a|s) - \pi(a|s)) \mathcal{P}(s'|s,a) V_{\pi', \mathcal{M}}(s') dads' \\
          \leq & 2r^* D_{\mathbf{TD}}(\pi,\pi')(s)  + \gamma \int (\pi'(a|s) - \pi(a|s)) \mathcal{P}(s'|s,a) V_{\pi', \mathcal{M}}(s') dads' \\
          = & 2r^* D_{\mathbf{TD}}(\pi,\pi')(s)  + \gamma \int (\pi'(a|s) - \pi(a|s)) \mathcal{P}(s'|s,a) (V_{\pi', \mathcal{M}}(s') - \bar{V}_{\mathcal{M},\pi}) dads' \\
          \leq & 2r^* D_{\mathbf{TD}}(\pi,\pi')(s)  + \gamma \int |\pi'(a|s) - \pi(a|s)| \mathcal{P}(s'|s,a) \frac{\hat{V}_{\mathcal{M},\pi}}{2} dads' \\
          = & 2r^* D_{\mathbf{TD}}(\pi,\pi')(s)  + \gamma \int |\pi'(a|s) - \pi(a|s)|  \frac{\hat{V}_{\mathcal{M},\pi}}{2} da \\
          \overset{(*1)}{\leq} & 2r^* D_{\mathbf{TD}}(\pi,\pi')(s)  +  \gamma D_{\mathbf{TD}}(\pi,\pi')(s)  \hat{V}_{\mathcal{M},\pi} \\
          \leq & 2r^* D_{\mathbf{TD}}(\pi,\pi')(s)  +  \gamma D_{\mathbf{TD}}(\pi,\pi')(s)  2\max_s|V_{\mathcal{M},\pi}(s)| \\
          \leq & 2r^* D_{\mathbf{TD}}(\pi,\pi')(s)  +  2\gamma D_{\mathbf{TD}}(\pi,\pi')(s)  \frac{1}{1-\gamma}r^* \\
          \overset{(*2)}{=} & \left(2 + \frac{2\gamma}{1-\gamma}\right) r^* D_{\mathbf{TD}}(\pi,\pi')(s).
      \end{split}
      \end{equation}
      $$
      这里式子$(*1)$即文章[5]中的定理2，式子$(*2)$即文章[6]中的定理5.<br>
      
      另一种特殊情况是我们假设两个MDP只有奖励函数相同，即只有$\mathcal{R}'=\mathcal{R}$而$\mathcal{P}'\neq \mathcal{P}$，此时
      $$
      \begin{equation}
      \begin{split}
          g(s) = & V_{\pi',\mathcal{M}'}(s) - \int \pi(a|s) \left[\mathcal{R}(s,a) +  \gamma\mathcal{P}(s'|s,a)V_{\pi', \mathcal{M}'}(s')\right] dads' \\
          = & \int \left[\pi'(a|s) - \pi(a|s) \right] \mathcal{R}(s,a) da \\
          + & \gamma \int  \left[ \pi'(a|s) \mathcal{P}'(s'|s,a)   - \pi(a|s) \mathcal{P}(s'|s,a) \right]  V_{\pi', \mathcal{M}'}(s') dads' \\
          \leq & 2r^* D_{\mathbf{TD}}(\pi,\pi')(s) + \gamma \int  \left[ \pi'(a|s) \mathcal{P}'(s'|s,a)   - \pi(a|s) \mathcal{P}(s'|s,a) \right]  V_{\pi', \mathcal{M}'}(s') dads' \\
          = & 2r^* D_{\mathbf{TD}}(\pi,\pi')(s) + \gamma \int  \pi'(a|s) \left[  \mathcal{P}'(s'|s,a) -  \mathcal{P}(s'|s,a) \right]  V_{\pi', \mathcal{M}'}(s') dads' \\
          + & \gamma \int  \left[ \pi'(a|s)  - \pi(a|s)\right] \mathcal{P}(s'|s,a)   V_{\pi', \mathcal{M}'}(s') dads' \\
          \leq & 2r^* D_{\mathbf{TD}}(\pi,\pi')(s) + \gamma \int  \pi'(a|s) |  \mathcal{P}'(s'|s,a) -  \mathcal{P}(s'|s,a) |  \max_{s''}| V_{\pi', \mathcal{M}'}(s'')| dads' \\
          + & \gamma \int  | \pi'(a|s)  - \pi(a|s)| \mathcal{P}(s'|s,a)   \max_{s''}| V_{\pi', \mathcal{M}'}(s'')|  dads' \\
          \leq & 2r^* D_{\mathbf{TD}}(\pi,\pi')(s) + \frac{\gamma}{1-\gamma} r^* \int  \pi'(a|s) |  \mathcal{P}'(s'|s,a) -  \mathcal{P}(s'|s,a) | dads' \\
          + & \frac{\gamma}{1-\gamma} r^* \int  | \pi'(a|s)  - \pi(a|s)| \mathcal{P}(s'|s,a) dads' \\
          = & 2r^* D_{\mathbf{TD}}(\pi,\pi')(s) + \frac{2\gamma}{1-\gamma} r^* D_{\mathbf{TD}}(\mathcal{P},\mathcal{P}')(s,a)  
          + \frac{2\gamma}{1-\gamma} r^* D_{\mathbf{TD}}(\pi,\pi')(s) \\
          \leq & \frac{2}{1-\gamma}r^* D_{\mathbf{TD}}(\pi,\pi')(s) + \frac{2\gamma}{1-\gamma} r^* D_{\mathbf{TD}}(\mathcal{P},\mathcal{P}')(s,a) 
      \end{split}
      \end{equation}
      $$
      这就是MBPO [7]中的引理3。
   </p>
</section>



<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{weng2018PG,
      title   = "Policy Gradient Algorithms",
      author  = "Weng, Lilian",
      journal = "lilianweng.github.io",
      year    = "2018",
      url     = "https://lilianweng.github.io/posts/2018-04-08-policy-gradient/"
    }
</code></pre>
  </div>
</section> -->

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
<h1 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h1>
<p>[1] Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. <a href="https://proceedings.neurips.cc/paper_files/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf">&ldquo;Policy gradientmethods for reinforcement learning with function approximation.&ldquo;</a> Advances in neural informationprocessing systems, 12, 1999.</p>
<p>[2] Sham Kakade and John Langford. <a href="https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/KakadeLangford-icml2002.pdf">&ldquo;</a> Approximately optimal approximate reinforcement learning.&ldquo;In Proceedings of the Nineteenth International Conference on Machine Learning, pages 267–274,2002.</p>
<p>[3] John Schulman, et al. <a href="https://arxiv.org/pdf/1502.05477.pdf">&ldquo;Trust region policy optimization.&rdquo;</a> In International conference on machine learning,. 2015.</p>
<p>[4] Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. <a href="https://arxiv.org/pdf/1702.08165">&ldquo;Reinforcement learning withdeep energy-based policies.&rdquo;</a> In International conference on machine learning, pages 1352–1361.PMLR, 2017.</p>
<p>[5] Chengyang Ying, Xinning Zhou, Hang Su, Dong Yan, Ning Chen, and Jun Zhu. <a href="https://arxiv.org/pdf/2206.04436">&ldquo;Towards safe re-inforcement learning via constraining conditional value-at-risk.&rdquo;</a> arXiv preprint arXiv:2206.04436,2022.</p>
<p>[6] Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Mingyan Liu, Duane Boning, and Cho-JuiHsieh. <a href="https://arxiv.org/pdf/2003.08938">&ldquo;Robust deep reinforcement learning against adversarial perturbations on state observations.&rdquo;</a> Advances in Neural Information Processing Systems, 33:21024–21037, 2020.</p>
<p>[7] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. <a href="https://arxiv.org/pdf/1906.08253">&ldquo;When to trust your model: Model-based policy optimization.&rdquo;</a> Advances in neural information processing systems, 32, 2019.</p>
</section>



<footer class="footer">
  <div class="container">
    <!-- <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div> -->
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is constructed using the source code provided by <a
            href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, and we are grateful for their template.
            <!-- This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>. -->
          </p>
          <!-- <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p> -->
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>